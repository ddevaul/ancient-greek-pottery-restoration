{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.VaseDataset import VaseDataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if we need transformations?\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), \n",
    "    transforms.ToTensor(), # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Make datasets\n",
    "train_dataset = VaseDataset(root_dir=\"dataset/train\", captions_file=\"captions.csv\", transform=transform)\n",
    "\n",
    "val_dataset = VaseDataset(root_dir=\"dataset/val\", captions_file=\"captions.csv\", transform=transform)\n",
    "\n",
    "\n",
    "# Make dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'masked_images': tensor([[[0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         ...,\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8118, 0.8118, 0.8157]],\n",
      "\n",
      "        [[0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         ...,\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8118, 0.8118, 0.8157]],\n",
      "\n",
      "        [[0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         ...,\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8118, 0.8118, 0.8157]]]), 'full_images': tensor([[[0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         ...,\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8118, 0.8078, 0.8157]],\n",
      "\n",
      "        [[0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         ...,\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8118, 0.8078, 0.8157]],\n",
      "\n",
      "        [[0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.9020, 0.9020, 0.9020,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         ...,\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8157, 0.8157, 0.8157],\n",
      "         [0.8667, 0.8667, 0.8667,  ..., 0.8118, 0.8078, 0.8157]]]), 'masks': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'text': 'Amphora Black'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholaskhorasani/Documents/Columbia_University/Fall2024/COMS4995-Deep-Learning-For-Computer-Vision/PotteryRestoration/dataset/VaseDataset.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mask = torch.load(mask_path)  # Load the mask tensor (H, W)\n"
     ]
    }
   ],
   "source": [
    "for val in val_dataset:\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholaskhorasani/Documents/Columbia_University/Fall2024/COMS4995-Deep-Learning-For-Computer-Vision/PotteryRestoration/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 17.47it/s]\n",
      "/Users/nicholaskhorasani/Documents/Columbia_University/Fall2024/COMS4995-Deep-Learning-For-Computer-Vision/PotteryRestoration/venv/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m\n\u001b[1;32m     24\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(\n\u001b[1;32m     25\u001b[0m     [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: pipe\u001b[38;5;241m.\u001b[39munet\u001b[38;5;241m.\u001b[39mparameters()}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: pipe\u001b[38;5;241m.\u001b[39mtext_encoder\u001b[38;5;241m.\u001b[39mparameters()}],\n\u001b[1;32m     26\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# DataLoader placeholder (replace `train_dataloader` with your actual DataLoader)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# train_dataloader = DataLoader(...)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Use Accelerator for distributed training\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m accelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m pipe, optimizer, train_dataloader \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mprepare(pipe, optimizer, train_dataloader)\n\u001b[1;32m     36\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Columbia_University/Fall2024/COMS4995-Deep-Learning-For-Computer-Vision/PotteryRestoration/venv/lib/python3.12/site-packages/accelerate/accelerator.py:485\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    483\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    484\u001b[0m ):\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    486\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# Load pipeline\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-inpainting\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Freeze VAE parameters\n",
    "for param in pipe.vae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Fine-tune only the U-Net and text encoder\n",
    "for param in pipe.unet.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in pipe.text_encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "pipe.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    [{\"params\": pipe.unet.parameters()}, {\"params\": pipe.text_encoder.parameters()}],\n",
    "    lr=5e-5\n",
    ")\n",
    "\n",
    "# DataLoader placeholder (replace `train_dataloader` with your actual DataLoader)\n",
    "# train_dataloader = DataLoader(...)\n",
    "\n",
    "# Use Accelerator for distributed training\n",
    "accelerator = Accelerator()\n",
    "pipe, optimizer, train_dataloader = accelerator.prepare(pipe, optimizer, train_dataloader)\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    pipe.unet.train()\n",
    "    pipe.text_encoder.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Get inputs\n",
    "        masked_images = batch[\"masked_images\"].to(device)\n",
    "        full_images = batch[\"full_images\"].to(device)\n",
    "        masks = batch[\"masks\"].to(device)  # Binary masks\n",
    "        prompts = batch[\"text\"]\n",
    "\n",
    "        # Tokenize text prompts\n",
    "        tokenized_prompts = pipe.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        text_embeddings = pipe.text_encoder(**tokenized_prompts).last_hidden_state\n",
    "\n",
    "        # Encode masked images into latent space\n",
    "        latents = pipe.vae.encode(masked_images).latent_dist.sample()\n",
    "        latents = latents * pipe.vae.config.scaling_factor\n",
    "\n",
    "        # Assert latent dimensions\n",
    "        assert latents.shape[1] == 4, f\"Latent channels should be 4, got {latents.shape[1]}\"\n",
    "        assert latents.shape[2] % 8 == 0 and latents.shape[3] % 8 == 0, \\\n",
    "            \"Latent dimensions should be divisible by 8 for the UNet\"\n",
    "\n",
    "        # Add noise to the latents\n",
    "        batch_size = latents.size(0)\n",
    "        timesteps = torch.randint(0, pipe.scheduler.num_train_timesteps, (batch_size,), device=device).long()\n",
    "        noise = torch.randn_like(latents)\n",
    "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Resize mask to match latent spatial dimensions\n",
    "        latent_masks = torch.nn.functional.interpolate(masks, size=noisy_latents.shape[-2:])  # Resize mask\n",
    "        latent_masks = latent_masks[:, None, :, :]  # Ensure shape is (B, 1, H, W)\n",
    "        \n",
    "        # Fix latent_masks shape by squeezing the extra singleton dimension\n",
    "        if latent_masks.ndim == 5:  # Check if there's an extra dimension\n",
    "            latent_masks = latent_masks.squeeze(2)  # Remove the extra dimension\n",
    "        # Assert mask shape matches expected dimensions\n",
    "        print(f\"Noisy latents shape: {noisy_latents.shape}\")\n",
    "        print(f\"Latent masks shape: {latent_masks.shape}\")\n",
    "        \n",
    "        assert latent_masks.ndim == 4, f\"Mask should have 4 dimensions, got {latent_masks.ndim}\"\n",
    "        assert latent_masks.shape[1] == 1, f\"Mask must have 1 channel, got {latent_masks.shape[1]}\"\n",
    "        assert latent_masks.shape[2:] == noisy_latents.shape[2:], \\\n",
    "            f\"Mask spatial dimensions {latent_masks.shape[2:]} must match latents {noisy_latents.shape[2:]}\"\n",
    "\n",
    "        # Generate spatial encodings\n",
    "        batch_size, _, height, width = noisy_latents.shape\n",
    "        x = torch.linspace(-1, 1, steps=width, device=device).view(1, 1, 1, -1).expand(batch_size, 1, height, width)\n",
    "        y = torch.linspace(-1, 1, steps=height, device=device).view(1, 1, -1, 1).expand(batch_size, 1, height, width)\n",
    "        spatial_encodings = torch.cat([x, y], dim=1)  # Shape: (B, 2, H, W)\n",
    "\n",
    "        # Concatenate noisy latents, mask, and spatial encodings\n",
    "        unet_input = torch.cat([noisy_latents, latent_masks, spatial_encodings], dim=1)\n",
    "\n",
    "        # Add extra dummy channels (if required)\n",
    "        extra_channels = torch.zeros(unet_input.shape[0], 2, unet_input.shape[2], unet_input.shape[3], device=device)\n",
    "        unet_input = torch.cat([unet_input, extra_channels], dim=1)\n",
    "\n",
    "        # Assert the input shape\n",
    "        assert unet_input.shape[1] == 9, f\"UNet input must have 9 channels, got {unet_input.shape[1]}\"\n",
    "\n",
    "        # Forward pass through UNet\n",
    "        unet_output = pipe.unet(\n",
    "            sample=unet_input,\n",
    "            timestep=timesteps,\n",
    "            encoder_hidden_states=text_embeddings\n",
    "        ).sample\n",
    "\n",
    "        # Assert UNet output shape matches latent input\n",
    "        assert unet_output.shape == latents.shape, \\\n",
    "            f\"UNet output shape mismatch: {unet_output.shape} != {latents.shape}\"\n",
    "\n",
    "        # Decode the output latents back to image space\n",
    "        reconstructed_images = pipe.vae.decode(unet_output / pipe.vae.config.scaling_factor).sample\n",
    "\n",
    "        # Assert decoded images match the size of full images\n",
    "        assert reconstructed_images.shape == full_images.shape, \\\n",
    "            f\"Decoded images shape mismatch: {reconstructed_images.shape} != {full_images.shape}\"\n",
    "\n",
    "        # Compute pixel-wise loss\n",
    "        loss = torch.nn.functional.mse_loss(reconstructed_images, full_images)\n",
    "\n",
    "        # Backpropagation\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} completed. Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
