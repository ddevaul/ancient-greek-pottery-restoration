{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the below two cells to run shell commands in colab without having to pay for Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to run shell commands in Google Colab\n",
    "from IPython.display import JSON\n",
    "from google.colab import output\n",
    "from subprocess import getoutput\n",
    "import os\n",
    "\n",
    "def shell(command):\n",
    "  if command.startswith('cd'):\n",
    "    path = command.strip().split(maxsplit=1)[1]\n",
    "    os.chdir(path)\n",
    "    return JSON([''])\n",
    "  return JSON([getoutput(command)])\n",
    "output.register_callback('shell', shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Colab Shell\n",
    "%%html\n",
    "<div id=term_demo></div>\n",
    "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
    "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
    "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
    "<script>\n",
    "  $('#term_demo').terminal(async function(command) {\n",
    "      if (command !== '') {\n",
    "          try {\n",
    "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
    "              let out = res.data['application/json'][0]\n",
    "              this.echo(new String(out))\n",
    "          } catch(e) {\n",
    "              this.error(new String(e));\n",
    "          }\n",
    "      } else {\n",
    "          this.echo('');\n",
    "      }\n",
    "  }, {\n",
    "      greetings: 'Welcome to Colab Shell',\n",
    "      name: 'colab_demo',\n",
    "      height: 250,\n",
    "      prompt: 'colab > '\n",
    "  });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ddevaul/ancient-greek-pottery-restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we are running on google colab or locally\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    print(\"Running on Google Colab\")\n",
    "    colab = True\n",
    "except ImportError:\n",
    "    print(\"Running locally\")\n",
    "    colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXNLSIfKsDGG"
   },
   "source": [
    "# Masking Images Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IYesihlTtMss"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# If running start from the main directory\n",
    "main_path = \"drive/MyDrive/Greek_Pottery_In_Painting/\" if colab else \"./\"\n",
    "\n",
    "specific_data = \"November_21_Images/\"\n",
    "full_path = main_path + specific_data + \"images.zip\"\n",
    "BASE_DIR = \"./images\"\n",
    "with zipfile.ZipFile(full_path, \"r\") as zip:\n",
    "    zip.extractall(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXLNaj3QtWS6",
    "outputId": "ba6e4fac-06f7-4b94-b540-914958d991a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302189-BLACK-FIGURE.jpg\n",
      "Total Number of Images: 6714\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "path_to_sub_dir = BASE_DIR + \"/\" + os.listdir(BASE_DIR)[0]\n",
    "image_names = [image_name for image_name in os.listdir(path_to_sub_dir)]\n",
    "random.shuffle(image_names)\n",
    "print(image_names[0])\n",
    "print(f\"Total Number of Images: {len(image_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>image_url</th>\n",
       "      <th>pottery_style</th>\n",
       "      <th>pottery_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images-nicky-test/208223-BLACK-FIGURE.jpg</td>\n",
       "      <td>http://www.beazley.ox.ac.uk/record/8A9E1A2D-DE...</td>\n",
       "      <td>BLACK-FIGURE</td>\n",
       "      <td>LEKYTHOS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image_path  \\\n",
       "0  images-nicky-test/208223-BLACK-FIGURE.jpg   \n",
       "\n",
       "                                           image_url pottery_style  \\\n",
       "0  http://www.beazley.ox.ac.uk/record/8A9E1A2D-DE...  BLACK-FIGURE   \n",
       "\n",
       "  pottery_shape  \n",
       "0      LEKYTHOS  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = [\"image_path\", \"image_url\", \"pottery_style\", \"pottery_shape\"]\n",
    "aggregate_data_df = pd.read_csv(\n",
    "    f\"{main_path + specific_data}/aggregrate_data.csv\", names=column_names\n",
    ")\n",
    "aggregate_data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_prefix = \"BF_image\"\n",
    "aggregate_data_df['uniform_image_name'] = [f\"{new_image_prefix}{i}.jpg\" for i in range(len(aggregate_data_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>image_url</th>\n",
       "      <th>pottery_style</th>\n",
       "      <th>pottery_shape</th>\n",
       "      <th>uniform_image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images-nicky-test/208223-BLACK-FIGURE.jpg</td>\n",
       "      <td>http://www.beazley.ox.ac.uk/record/8A9E1A2D-DE...</td>\n",
       "      <td>BLACK-FIGURE</td>\n",
       "      <td>LEKYTHOS</td>\n",
       "      <td>BF_image0.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image_path  \\\n",
       "0  images-nicky-test/208223-BLACK-FIGURE.jpg   \n",
       "\n",
       "                                           image_url pottery_style  \\\n",
       "0  http://www.beazley.ox.ac.uk/record/8A9E1A2D-DE...  BLACK-FIGURE   \n",
       "\n",
       "  pottery_shape uniform_image_name  \n",
       "0      LEKYTHOS      BF_image0.jpg  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "image_train_base_path = (\n",
    "    \"./PotteryRestoration/dataset/train/original_images\"\n",
    "    if colab\n",
    "    else \"./dataset/train/original_images\"\n",
    ")\n",
    "os.makedirs(image_train_base_path, exist_ok=True)\n",
    "\n",
    "for index, row in aggregate_data_df.iterrows():\n",
    "    original_image_path = row[\"image_path\"]\n",
    "    new_name = row[\"uniform_image_name\"]\n",
    "    shutil.copy(\n",
    "        f\"{BASE_DIR}/{original_image_path}\", f\"{image_train_base_path}/{new_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our aggregate data to the correct directory (We can just use this as \"captions.csv\")\n",
    "aggregate_data_location = (\n",
    "    \"./PotteryRestoration/dataset/train/aggregate_data.csv\"\n",
    "    if colab\n",
    "    else \"./dataset/train/aggregate_data.csv\"\n",
    ")\n",
    "aggregate_data_df.to_csv(aggregate_data_location, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Y1MiCD2RIXu",
    "outputId": "be7afe7b-7e59-4a98-e56c-4aa4336bbdc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating masks for 6716 images using 9 CPUs...\n",
      "Processing Images: 100%|████████████████████████| 34/34 [00:47<00:00,  1.39s/it]\n",
      "Saved mappings to ./dataset/train/mask_mappings.csv\n"
     ]
    }
   ],
   "source": [
    "mask_images_code_path = \"./PotteryRestoration/dataset/mask_images.py\" if colab else \"./dataset/mask_images.py\"\n",
    "!python {mask_images_code_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Images Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Unzip the Masked Dataset to the correct directory\n",
    "main_path = \"/content/drive/MyDrive/Greek_Pottery_In_Painting/\" if colab else \"./\"\n",
    "\n",
    "specific_data = \"November_24_Dataset/\"\n",
    "full_path = main_path + specific_data + \"train.zip\"\n",
    "BASE_DIR = \"./ancient-greek-pottery-restoration/dataset\"\n",
    "with zipfile.ZipFile(full_path, \"r\") as zip:\n",
    "    zip.extractall(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 53728\n",
      "Validation size: 6716\n",
      "Test size: 6716\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from importlib import import_module\n",
    "\n",
    "# Add to sys path since can't import with dashs\n",
    "sys.path.append('./ancient-greek-pottery-restoration/dataset' if colab else './dataset')\n",
    "\n",
    "# Import the VaseDataset dynamically\n",
    "VaseDataset = import_module(\"VaseDataset\").VaseDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "# We can try 512 x 512 later but it takes much more GPU Ram and training time\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_root_dir = \"./ancient-greek-pottery-restoration/dataset/train\" if colab else \"./dataset/train\"\n",
    "agg_data_file_name = \"aggregate_data.csv\"\n",
    "mask_mappings_file_path = \"mask_mappings.csv\"\n",
    "\n",
    "\n",
    "dataset = VaseDataset(\n",
    "    dataset_root_dir=dataset_root_dir,\n",
    "    agg_data_file_name=agg_data_file_name,\n",
    "    mask_mappings_file_path=mask_mappings_file_path,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_fraction = 0.8\n",
    "val_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "# Get size of each split\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * train_fraction)\n",
    "val_size = int(total_size * val_fraction)\n",
    "test_size = total_size - train_size - val_size # Make sure the sizes add up\n",
    "\n",
    "# Perform the split\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Make data loaders for each of our datasets\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check on the sizes of each dataset\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "print(batch['masked_images'].shape)\n",
    "print(batch['full_images'].shape)\n",
    "print(batch['masks'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some online resources for training unets\n",
    "- \n",
    "- https://huggingface.co/learn/diffusion-course/en/unit2/2\n",
    "- https://github.com/huggingface/diffusers/discussions/8458\n",
    "- https://discuss.huggingface.co/t/fine-tuning-controlnet-xs-with-sdxl/92652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d673dd481f8f40d48d67e6eb1905d813",
      "a94fa1a272c347aeb46ae8a7e00b56e0",
      "6a86f5b34083487e91a8da12602a41c4",
      "f9335e35b76447ef8805b0ada65beea4",
      "c29c17f40b78444095844de0acca2858",
      "33cd398485864931947e61e99fd397a4",
      "9aab566a9ab946dabfe4ac77a735062b",
      "c71f6572025c4aa1be480b4fe9a04e56",
      "93a14769cac7495d9d88bcd178ae3ec1",
      "4dd02126b2da47c8a0a62e021a0330cc",
      "8c4836f255fc46e59b0847e4f2026b1d"
     ]
    },
    "id": "_NyeCkFpO8Wc",
    "outputId": "81ff2409-d5da-4fe6-ec8b-ccfb22239817"
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# Load pipeline\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-inpainting\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Freeze VAE parameters\n",
    "for param in pipe.vae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Fine-tune only the U-Net and text encoder\n",
    "for param in pipe.unet.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in pipe.text_encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "pipe.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    [{\"params\": pipe.unet.parameters()}, {\"params\": pipe.text_encoder.parameters()}],\n",
    "    lr=5e-5\n",
    ")\n",
    "\n",
    "# DataLoader placeholder (replace `train_dataloader` with your actual DataLoader)\n",
    "# train_dataloader = DataLoader(...)\n",
    "\n",
    "# Use Accelerator for distributed training\n",
    "accelerator = Accelerator()\n",
    "pipe, optimizer, train_dataloader = accelerator.prepare(pipe, optimizer, train_loader)\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "log_interval = 10  \n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    pipe.unet.train()\n",
    "    pipe.text_encoder.train()\n",
    "\n",
    "    epoch_loss = 0  \n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # Get inputs\n",
    "        masked_images = batch[\"masked_images\"].to(device)\n",
    "        full_images = batch[\"full_images\"].to(device)\n",
    "        masks = batch[\"masks\"].to(device)  # Binary masks\n",
    "        prompts = batch[\"text\"]\n",
    "\n",
    "        # Tokenize text prompts\n",
    "        tokenized_prompts = pipe.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        text_embeddings = pipe.text_encoder(**tokenized_prompts).last_hidden_state\n",
    "\n",
    "        # Encode masked images into latent space\n",
    "        latents = pipe.vae.encode(masked_images).latent_dist.sample()\n",
    "        latents = latents * pipe.vae.config.scaling_factor\n",
    "\n",
    "        # Assert latent dimensions\n",
    "        assert latents.shape[1] == 4, f\"Latent channels should be 4, got {latents.shape[1]}\"\n",
    "        assert latents.shape[2] % 8 == 0 and latents.shape[3] % 8 == 0, \\\n",
    "            \"Latent dimensions should be divisible by 8 for the UNet\"\n",
    "\n",
    "        # Add noise to the latents\n",
    "        batch_size = latents.size(0)\n",
    "        timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (batch_size,), device=device).long()\n",
    "        noise = torch.randn_like(latents)\n",
    "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Resize mask to match latent spatial dimensions\n",
    "        latent_masks = torch.nn.functional.interpolate(masks, size=noisy_latents.shape[-2:])  # Resize mask\n",
    "        latent_masks = latent_masks[:, None, :, :]  # Ensure shape is (B, 1, H, W)\n",
    "\n",
    "        # Fix latent_masks shape by squeezing the extra singleton dimension\n",
    "        if latent_masks.ndim == 5:  # Check if there's an extra dimension\n",
    "            latent_masks = latent_masks.squeeze(2)  # Remove the extra dimension\n",
    "        # Assert mask shape matches expected dimensions\n",
    "        # print(f\"Noisy latents shape: {noisy_latents.shape}\")\n",
    "        # print(f\"Latent masks shape: {latent_masks.shape}\")\n",
    "\n",
    "        assert latent_masks.ndim == 4, f\"Mask should have 4 dimensions, got {latent_masks.ndim}\"\n",
    "        assert latent_masks.shape[1] == 1, f\"Mask must have 1 channel, got {latent_masks.shape[1]}\"\n",
    "        assert latent_masks.shape[2:] == noisy_latents.shape[2:], \\\n",
    "            f\"Mask spatial dimensions {latent_masks.shape[2:]} must match latents {noisy_latents.shape[2:]}\"\n",
    "\n",
    "        # Generate spatial encodings\n",
    "        batch_size, _, height, width = noisy_latents.shape\n",
    "        x = torch.linspace(-1, 1, steps=width, device=device).view(1, 1, 1, -1).expand(batch_size, 1, height, width)\n",
    "        y = torch.linspace(-1, 1, steps=height, device=device).view(1, 1, -1, 1).expand(batch_size, 1, height, width)\n",
    "        spatial_encodings = torch.cat([x, y], dim=1)  # Shape: (B, 2, H, W)\n",
    "\n",
    "        # Concatenate noisy latents, mask, and spatial encodings\n",
    "        unet_input = torch.cat([noisy_latents, latent_masks, spatial_encodings], dim=1)\n",
    "\n",
    "        # Add extra dummy channels (if required)\n",
    "        extra_channels = torch.zeros(unet_input.shape[0], 2, unet_input.shape[2], unet_input.shape[3], device=device)\n",
    "        unet_input = torch.cat([unet_input, extra_channels], dim=1)\n",
    "\n",
    "        # Assert the input shape\n",
    "        assert unet_input.shape[1] == 9, f\"UNet input must have 9 channels, got {unet_input.shape[1]}\"\n",
    "\n",
    "        # Forward pass through UNet\n",
    "        unet_output = pipe.unet(\n",
    "            sample=unet_input,\n",
    "            timestep=timesteps,\n",
    "            encoder_hidden_states=text_embeddings\n",
    "        ).sample\n",
    "\n",
    "        # Assert UNet output shape matches latent input\n",
    "        assert unet_output.shape == latents.shape, \\\n",
    "            f\"UNet output shape mismatch: {unet_output.shape} != {latents.shape}\"\n",
    "\n",
    "        # Decode the output latents back to image space\n",
    "        reconstructed_images = pipe.vae.decode(unet_output / pipe.vae.config.scaling_factor).sample\n",
    "\n",
    "        # Assert decoded images match the size of full images\n",
    "        assert reconstructed_images.shape == full_images.shape, \\\n",
    "            f\"Decoded images shape mismatch: {reconstructed_images.shape} != {full_images.shape}\"\n",
    "\n",
    "        # Compute pixel-wise loss\n",
    "        loss = torch.nn.functional.mse_loss(reconstructed_images, full_images)\n",
    "\n",
    "        # Backpropagation\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Log progress\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Batch {batch_idx + 1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        del masked_images, full_images, masks, tokenized_prompts, latents, noisy_latents, latent_masks, unet_input, unet_output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} completed. Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "33cd398485864931947e61e99fd397a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4dd02126b2da47c8a0a62e021a0330cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a86f5b34083487e91a8da12602a41c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c71f6572025c4aa1be480b4fe9a04e56",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93a14769cac7495d9d88bcd178ae3ec1",
      "value": 6
     }
    },
    "8c4836f255fc46e59b0847e4f2026b1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93a14769cac7495d9d88bcd178ae3ec1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9aab566a9ab946dabfe4ac77a735062b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a94fa1a272c347aeb46ae8a7e00b56e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33cd398485864931947e61e99fd397a4",
      "placeholder": "​",
      "style": "IPY_MODEL_9aab566a9ab946dabfe4ac77a735062b",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "c29c17f40b78444095844de0acca2858": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c71f6572025c4aa1be480b4fe9a04e56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d673dd481f8f40d48d67e6eb1905d813": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a94fa1a272c347aeb46ae8a7e00b56e0",
       "IPY_MODEL_6a86f5b34083487e91a8da12602a41c4",
       "IPY_MODEL_f9335e35b76447ef8805b0ada65beea4"
      ],
      "layout": "IPY_MODEL_c29c17f40b78444095844de0acca2858"
     }
    },
    "f9335e35b76447ef8805b0ada65beea4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4dd02126b2da47c8a0a62e021a0330cc",
      "placeholder": "​",
      "style": "IPY_MODEL_8c4836f255fc46e59b0847e4f2026b1d",
      "value": " 6/6 [00:00&lt;00:00,  6.99it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
